{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (4.14.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (4.27.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: sacremoses in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from transformers) (0.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.6)\n",
      "Requirement already satisfied: six in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/wing.nus/.pyenv/versions/3.8.0/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import json\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "from functools import partial\n",
    "\n",
    "from pytorch_transformers import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_example = ['12.14AM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embedding_type = \"bert-base-cased\"\n",
    "do_lower_case = \"uncased\" in embedding_type\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_type, do_lower_case=do_lower_case, add_special_tokens=False)\n",
    "bert_embeddings = AutoModel.from_pretrained(embedding_type, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12', '.', '14', '##AM']\n",
      "{'input_ids': tensor([[  101,  1367,   119,  1489, 10964,   102,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0]])}\n",
      "[CLS] 12. 14AM [SEP] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(sentence_example, is_split_into_words=True)\n",
    "bert_tokens = tokenizer(sentence_example, return_tensors=\"pt\", is_split_into_words=True,\n",
    "                                                 padding=\"max_length\", truncation=True, max_length=8)\n",
    "print(tokens)\n",
    "print(bert_tokens)\n",
    "print(tokenizer.decode(bert_tokens['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 768]) 10\n"
     ]
    }
   ],
   "source": [
    "sub_tokens_map = {}\n",
    "total_subtokens = 0\n",
    "for text in sentence_example:\n",
    "    subtokens_length = len(tokenizer.tokenize(text))\n",
    "    sub_tokens_map[text] = subtokens_length\n",
    "    total_subtokens += subtokens_length\n",
    "\n",
    "bert_tokens = tokenizer(sentence_example, return_tensors=\"pt\", is_split_into_words=True,\n",
    "                                                 padding=\"max_length\", truncation=True, max_length=10)\n",
    "padding_length = len(bert_tokens['input_ids'][0]) - total_subtokens - 2 # plus [CLS] and [SEP]\n",
    "\n",
    "with torch.no_grad():\n",
    "    bert_output = bert_embeddings(**bert_tokens) # 1, max_length, 768\n",
    "\n",
    "sum_all_layers = sum(bert_output.hidden_states[0:12])[0] # changed this\n",
    "if padding_length > 0:\n",
    "    sum_all_layers = sum_all_layers[:-padding_length]\n",
    "sum_all_layers = sum_all_layers[1:len(sum_all_layers) - 1] # exclude [CLS] and [SEP]\n",
    "\n",
    "index = 0\n",
    "embeddings = []\n",
    "for text in sentence_example:\n",
    "    bert_embedding = sum_all_layers[index]\n",
    "    for i in range(1, sub_tokens_map[text]):\n",
    "        bert_embedding += sum_all_layers[index + i]\n",
    "    bert_embedding /= sub_tokens_map[text]\n",
    "    embeddings.append(bert_embedding)\n",
    "    index += sub_tokens_map[text]\n",
    "while len(embeddings) < 10:\n",
    "    zeros = torch.zeros(768)\n",
    "    zeros = zeros\n",
    "    embeddings.append(zeros)\n",
    "print(torch.stack(embeddings).shape, len(torch.stack(embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
